version: '3.8'

services:
  deepseek-ocr:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: deepseek-ocr-app
    restart: unless-stopped
    ports:
      - "7860:7860"
    volumes:
      # Mount your model directory here
      # Update the left path to your actual model location
      - ./models:/app/models
      # Optional: mount for persistent data
      - ./data:/app/data
      # Optional: mount for logs
      - ./logs:/app/logs
    environment:
      # Model configuration
      - MODEL_PATH=/app/models/DeepSeek-OCR
      - CUDA_VISIBLE_DEVICES=0
      
      # Server configuration
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=7860
      
      # Python configuration
      - PYTHONUNBUFFERED=1
      - GRADIO_SERVER_NAME=0.0.0.0
      
    # GPU support - uncomment if you have nvidia-docker
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    # Alternative GPU support for older Docker versions
    # runtime: nvidia
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Optional: Add nginx reverse proxy
  # nginx:
  #   image: nginx:alpine
  #   container_name: deepseek-ocr-nginx
  #   restart: unless-stopped
  #   ports:
  #     - "80:80"
  #     - "443:443"
  #   volumes:
  #     - ./nginx/nginx.conf:/etc/nginx/nginx.conf
  #     - ./nginx/ssl:/etc/nginx/ssl
  #   depends_on:
  #     - deepseek-ocr

# Optional: Create named volumes
volumes:
  models:
    driver: local
  data:
    driver: local
  logs:
    driver: local

# Optional: Create custom network
networks:
  deepseek-network:
    driver: bridge