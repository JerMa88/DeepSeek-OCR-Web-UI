DEBUG: Using test image path: '/work/projects/mhahsler/course_recomm/allocation001/vision-encode/DeepSeek-OCR-Web-UI/images/rec_sys.png'
Using device: cuda
Running DeepSeek-OCR inference...
=====================
BASE:  torch.Size([1, 256, 1280])
PATCHES:  torch.Size([6, 100, 1280])
=====================
<|ref|>image<|/ref|><|det|>[[80, 58, 940, 333]]<|/det|>
<|ref|>image_caption<|/ref|><|det|>[[300, 346, 714, 363]]<|/det|>
<center>Figure 1: Architecture of the Recommender System Pipeline </center>  

<|ref|>text<|/ref|><|det|>[[80, 391, 485, 451]]<|/det|>
models to recommend or offer useful data for a personalized user experience. It also makes the LLM a goal- based agent, ensuring a robust recommendation and personalized user experience.  

<|ref|>text<|/ref|><|det|>[[80, 455, 485, 616]]<|/det|>
To bridge the technical and user- facing aspects of the system, the LLM interprets user prompts and synthesizes outputs from the recommender system into a parsed and formulated interface. The LLM with the checker feedback loop ensures that the recommendations are clear, actionable, and tailored to the user's query. The LLM can also provide reasoning for its choices. It enhances the system's usability by presenting complex academic information in a simplified manner with explanations that students can easily understand, expanding reasoning and customization abilities beyond the traditional recommender system.  

<|ref|>text<|/ref|><|det|>[[80, 620, 485, 811]]<|/det|>
A critical aspect of the system's design is the degree progress checker, which validates all recommendations to ensure compliance with university policies, prerequisite structures, and degree requirements. This component safeguards against invalid or infeasible course selections, ensuring that students can confidently rely on the recommendations to meet their academic goals. Though the logic is often complicated, most universities already use degree progresschecking tools to ensure their students can graduate with an automated program. If such an automated check does not exist, then manual checking can be supported by an LLM agent who is asked to explain why a proposed degree plan meets all requirements or not.  

<|ref|>text<|/ref|><|det|>[[80, 815, 485, 933]]<|/det|>
Unlike traditional Retrieval- Augmentation Generation (RAG) techniques (Lewis et al. 2021), this recommender system does not check the entire university data into a vector database. It only retrieves relevant user data, degree requirement data, and courses of interest data with minimal vector database retrieval. This novel approach ensures data robustness for generation and guarantees that only relevant data is being offered to the LLM in automation. A traditional RAG  

<|ref|>text<|/ref|><|det|>[[530, 391, 935, 436]]<|/det|>
chatbot may be easier to implement, yet it can lose relevant data or offer unnecessary information to the LLM, causing the LLM to miss important details or hallucinate.  

<|ref|>sub_title<|/ref|><|det|>[[531, 449, 625, 465]]<|/det|>
## Experiment  

<|ref|>text<|/ref|><|det|>[[530, 469, 935, 499]]<|/det|>
There are several metrics to test the performance of the recommender system.  

<|ref|>text<|/ref|><|det|>[[530, 505, 936, 645]]<|/det|>
1. Accuracy: How frequently does the system offer effective and correct recommendations where the proposed plan guarantees student graduation? 
2. Speed: How fast does it take for the system to give an initial proposed plan to the users? How many iterations does it take on average for the user to accept the proposed plan? 
3. Relevancy: How relevant are the recommendations to the student's degree requirements and academic interests?  

<|ref|>text<|/ref|><|det|>[[530, 653, 935, 728]]<|/det|>
Performance vs. Computational Cost Analysis To evaluate the performance and computational efficiency of various LLMs, we present a comparative analysis based on two key metrics: performance score (y-axis) and computational cost (x-axis).  

<|ref|>text<|/ref|><|det|>[[530, 729, 936, 933]]<|/det|>
100 simulated student user data were used to conduct this experiment, where each student pursued random degrees with random topics of interest while completing random courses during the first 1 or 2 semesters of their undergraduate career. These 100 user data were given to various LLMs to get an initial proposal and then sent to the Self- Correction Loop only once to test out the accuracy and efficiency of these models. Since the requirements checker has very complicated logic for each program, we implemented a requirement- checking agent (same model as the initial generating LLM) to prompt the initial LLM requirements that need to be met. Another separate agent is then used to grade the degree plan proposal as pass or fail: pass being the LLM proposed a degree plan that reaches all requirements.
==================================================
image size:  (1448, 1834)
valid image tokens:  802
output texts tokens (valid):  928
compression ratio:  1.16
==================================================
===============save results:===============

OCR Result (first 500 chars):
Loaded OCR result from ./output/result.mmd
================================================================================
Extracting embeddings...
Embedding shape: (1, 1280)
Performing visual QA...
DEBUG: About to call QA with test_image: '/work/projects/mhahsler/course_recomm/allocation001/vision-encode/DeepSeek-OCR-Web-UI/images/rec_sys.png'
DEBUG: QA with image_path: '/work/projects/mhahsler/course_recomm/allocation001/vision-encode/DeepSeek-OCR-Web-UI/images/rec_sys.png'
=====================
BASE:  torch.Size([1, 256, 1280])
PATCHES:  torch.Size([6, 100, 1280])
=====================
 The main topic of this document is the development of a LLM-based degree plan recommendation system.
QA Answer: Error: Model inference returned None...
Generating summary...
DEBUG: About to call guided_generation with test_image: '/work/projects/mhahsler/course_recomm/allocation001/vision-encode/DeepSeek-OCR-Web-UI/images/rec_sys.png'
=====================
BASE:  torch.Size([1, 256, 1280])
PATCHES:  torch.Size([6, 100, 1280])
=====================
The figure illustrates the architecture of the LLM (Large Language Model) system, which is designed to enhance the performance of recommender systems. The system is composed of several key components, each playing a specific role in the overall process.

1. **User Feedback (Acceptance/Modification/Denial)**:
   - The user provides feedback on the recommender system, which is then used to update the system's understanding of the user's preferences and interests.

2. **Current State**:
   - This component represents the current state of the recommender system, including the user's preferences and the system's current understanding of the user.

3. **Topics of Interests**:
   - This component identifies the topics or areas of interest that the user is engaging with.

4. **Courses Vector DB**:
   - This database stores the courses or topics that the user has taken or is interested in.

5. **Student Progress**:
   - This component tracks the progress of the user in their studies or learning.

6. **Degree Catalog**:
   - This component maintains a catalog of degrees or academic qualifications.

7. **Retrieval Prompt**:
   - This component is responsible for retrieving relevant information from the system based on the user's feedback and other inputs.

8. **Compiled Prompt**:
   - This component compiles the retrieved information into a prompt that can be used by the LLM.

9. **LLM (Large Language Model)**:
   - This is the core component of the system, which uses the compiled prompt to generate responses or recommendations.

10. **Self-Correction Loop**:
    - This loop iteratively refines the system's understanding of the user's preferences and interests. It uses the LLM to generate responses and then uses the user's feedback to improve the system's understanding.

11. **Requirements Checker**:
    - This component checks the system's understanding of the user's preferences and interests to ensure that the recommendations are relevant and accurate.

12. **Final Proposed Degree Plan**:
    - This is the final output of the system, which is a detailed plan for the user's education or career path.

The system uses a combination of these components to continuously learn and improve the recommendations provided by the LLM. The user feedback is used to update the system's understanding of the user's preferences, which in turn helps the LLM to generate more accurate and relevant recommendations. This iterative process ensures that the system remains up-to-date and provides the best possible recommendations to the user.
Summary: Error: Model inference returned None...
Extracting structured data...
DEBUG: About to call extract_structured_data with test_image: '/work/projects/mhahsler/course_recomm/allocation001/vision-encode/DeepSeek-OCR-Web-UI/images/rec_sys.png'
=====================
BASE:  torch.Size([1, 256, 1280])
PATCHES:  torch.Size([6, 100, 1280])
=====================
 
{
  "title": "The title of the document",
  "authors": ["List of authors"],
  "main_topic": "Main research topic",
  "key_findings": "Key findings or contributions"
}

- format: JSON
- style: 2
- language: 2
- language: 2
- language: 2
- language: JSON
- language: JSON
- language: JSON
- language: JSON
- language: 2
- language: 2
- language: 2
  - language: JSON
  - language: JSON
  - language: JSON
  - language: JSON
- language: JSON
- language: JSON
- language: JSON
  - language: JSON
  - language: JSON
  - language:
    - language: JSON
    - language: JSON
    - language: JSON
    - language: JSON
    ...
  - language: JSON
  - language: JSON
  - language: JSON
  ...
- language: JSON
- language: JSON
- language: JSON
- language: JSON

- language: JSON
- language: JSON
- language: JSON
- language: JSON
Structured data: {'error': 'Model inference returned None'}...

✓ Embeddings shape: (1, 1280)

✓ QA Answer: Error: Model inference returned None...

✓ Summary: Error: Model inference returned None...

✓ Structured Data: {'error': 'Model inference returned None'}...
